# -*- coding: utf-8 -*-
"""
Created on Mon Jun 18 13:06:06 2018

@author: bhupendarsingh
"""

# Data Preprocessing Template

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
#reading the data
df = pd.read_csv('KCPD_Crime_Data_2017.csv')
#columns information 
df.info()
#just column names
df.columns
#columns datatypes information  
df.dtypes
#column values
df.values
#Indexes
df.index
#top 5 rows of each column
df.head(2)
#last 5 rows of each column
df.tail(2)
df.shape
type(df)
df.nunique()
df.ftypes
df.get_dtype_counts()
df.get_ftype_counts()
df.select_dtypes(exclude=['object']).head(5)
df.select_dtypes(include=['object']).head(5)
df.get_values()
df.axes
df.ndim
#number of elements in this object rows*columns
df.size
df.empty
df.infer_objects().dtypes
df.isna().head(5)
df.notna().head(5)
df.loc[4].at['Report_No'] #df.head(5)

df.T
df.describe()
df.describe(include='all')
df.describe(include=np.object)
df.describe(include=np.number)
df.describe(exclude=np.object)

np.can_cast(    )

df['Report_No'].describe()
df['Offense'].describe()
df['Reported_Date'].describe()
df['Reported_Time'].describe()
df['Description'].describe()
df['Age'].describe()
df['Area'].describe()
df['City'].describe()
df['Zip Code'].describe()

df['Report_No'].isnull().sum()
df['Offense'].isnull().sum()
df['Reported_Date'].isnull().sum()
df['Reported_Time'].isnull().sum()
df['Age'].isnull().sum()
df['Area'].isnull().sum()
df['City'].isnull().sum()
df['Zip Code'].isnull().sum()


#count not null columns
dataset = pd.read_csv('KCPD_Crime_Data_2017.csv')
nnull_columns=df.columns[df.notnull().any()]
df[nnull_columns].notnull().sum()

#count null columns
dataset = pd.read_csv('KCPD_Crime_Data_2017.csv')
null_columns=df.columns[df.isnull().any()]
df[null_columns].isnull().sum()

#print single column is null
print(df[df["City"].isnull()][null_columns])

#print all null columns
print(df[df.isnull().any(axis=1)][null_columns].head())

#count unique in columns
dataset = pd.read_csv('KCPD_Crime_Data_2017.csv')
unique_columns=df.columns[df.any().unique()]
df[unique_columns].unique().sum()

pd.unique(df)
AR = df['City'].unique()
AR
mt = pd.crosstab(index=df["Report_No"],columns="count")
mt.head(5)

RNo_ZC = pd.crosstab(index=df["Report_No"],columns=df["Area"])
RNo_ZC.head(50)

#Total number of crimes
np.count_nonzero(df['Report_No'])

#int64
t=np.unique(df['Report_No'])
np.count_nonzero(np.unique(df['Offense']))


#column datatype
df['Report_No'].dtypes
#column rows
df['Report_No']
#column top 5 rows
df['Report_No'].head(5)
#column last 5 rows
df['Report_No'].tail(5)
#column total row counts
df['Report_No'].value_counts()
#column top row counts
df['Report_No'].value_counts().head()
#indexes
df['Report_No'].index
#Transpose 
df.T
#rows and columns
df[0:0]
df[:1]
df[:2]

df.iloc[0:0,0:0]
#slicing rows explicitly
df.iloc[1:3,:]
#slicing rows explicitly
df.iloc[:,1:3]


df.describe()

pieces=df.iloc[:,0],df.iloc[:,3],df.iloc[:,4],df.iloc[:,8],df.iloc[:,10],df.iloc[:,13],df.iloc[:,14]
dff= pd.concat(pieces)


Encoding to ordinal variables
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.metrics import log_loss
#df = pd.read_csv('KCPD_Crime_Data_2017.csv')
df = pd.read_csv('KCPD_Crime_Data_2017.csv', encoding='utf-8')
RN = np.unique(df['Report_No'])
AR = np.unique(df['City'])
AR

pd.
df['Area'].isnull().sum()

from sklearn.preprocessing import LabelEncoder
ofs = LabelEncoder()
of_labels = ofs.fit_transform(df['Offense'])
offense_mappings = {index: label for index, label in 
                  enumerate(ofs.classes_)}
offense_mappings

from sklearn.preprocessing import LabelEncoder
 = LabelEncoder()
ar_labels = ofs.fit_transform(df['Offense'])
offense_mappings = {index: label for index, label in 
                  enumerate(ofs.classes_)}
offense_mappings

df = "KCPD_Crime_Data_2017.csv"
dff = pd.read_csv(df)
msk = np.random.rand(len(df)) < 0.8
features = [3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23]
X_train = df[msk].iloc[:,features]
X_test = df[~msk].iloc[:,features]
y_train = df[msk].iloc[:,1]
y_test = df[~msk].iloc[:,1]
print(log_loss(y_test,np.ones(len(y_test))*y_train.mean()))



One hot encoding (or dummy variabales)
Feature hashing (a.k.a the hashing trick)


#quick statistics on data
df.describe()

# Importing the dataset
dataset = pd.read_csv('KCPD_Crime_Data_2017.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 3].values

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
"""from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train)"""
